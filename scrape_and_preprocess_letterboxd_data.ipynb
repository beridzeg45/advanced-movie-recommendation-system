{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "import time\n",
    "\n",
    "edge_options = Options()\n",
    "#edge_options.add_argument('--headless')\n",
    "edge_options.add_argument(\"--blink-settings=imagesEnabled=false\")  # Disable images\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "import os\n",
    "path=r\"C:\\Users\\berid\\python\\yts mx project\"\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "driver = webdriver.Edge(options=edge_options)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_letterboxd_urls=[]\n",
    "\n",
    "for page in range(1,501):\n",
    "    url=f\"https://letterboxd.com/films/popular/page/{page}/\"\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        url_elements=WebDriverWait(driver,10).until(expected_conditions.visibility_of_all_elements_located((By.CSS_SELECTOR,'ul[class=\"poster-list -p70 -grid\"] li[class=\"listitem poster-container\"] a[class=\"frame\"]')))\n",
    "        for url_element in url_elements:\n",
    "            try:\n",
    "                title=url_element.get_attribute('textContent')\n",
    "                url=url_element.get_attribute('href')\n",
    "                \n",
    "                name_url_dict={'Title':title,'URL':url}\n",
    "\n",
    "                all_letterboxd_urls.append(name_url_dict)\n",
    "            except:\n",
    "                continue\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    print(f'Page : {page}, Scraped : {len(all_letterboxd_urls)}',end='\\r')\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump(all_letterboxd_urls,open(os.path.join(path,'scraped_data','all_letterboxd_urls.pickle'),'wb'))\n",
    "all_letterboxd_urls=pickle.load(open(os.path.join(path,'scraped_data','all_letterboxd_urls.pickle'),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total letterboxd URLs : 35990\n"
     ]
    }
   ],
   "source": [
    "all_letterboxd_urls = list({d[\"URL\"]: d for d in all_letterboxd_urls}.values())\n",
    "print(f'Total letterboxd URLs : {len(all_letterboxd_urls)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yts_movie_titles=pd.read_csv(os.path.join(path,'scraped_data','cleaned_yts_movies_df.csv'))['TITLE'].str.strip().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_letterboxd_urls=[d['URL'] for d in all_letterboxd_urls if d['Title'].split(' (')[0].strip() in yts_movie_titles]\n",
    "#print(len(useful_letterboxd_urls)/len(yts_movie_titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_movie_details(url):\n",
    "\n",
    "    html=requests.get(url).content\n",
    "    soup=BeautifulSoup(html)\n",
    "\n",
    "    try:\n",
    "        title=soup.select_one('div[class=\"details\"] h1[class=\"headline-1 filmtitle\"]').text.replace('\\n','')\n",
    "    except:\n",
    "        title=None\n",
    "\n",
    "    try:\n",
    "        year=soup.select_one('div[class=\"details\"] div[class=\"releaseyear\"]').text.replace('\\n','')\n",
    "    except:\n",
    "        year=None\n",
    "\n",
    "    try:\n",
    "        original_name=soup.select_one('div[class=\"details\"] h2[class=\"originalname\"]').text.replace('\\n','')\n",
    "    except:\n",
    "        original_name=None\n",
    "\n",
    "    try:\n",
    "        imdb=soup.select_one('p[class=\"text-link text-footer\"] a[data-track-action=\"IMDb\"]')['href']\n",
    "    except:\n",
    "        imdb=None \n",
    "\n",
    "    try:\n",
    "        director=soup.select_one('div[class=\"details\"] span[class=\"directorlist\"]').text.replace('\\n','')\n",
    "    except:\n",
    "        director=None\n",
    "\n",
    "    try:\n",
    "        plot=soup.select_one('div[class=\"review body-text -prose -hero prettify\"]').text\n",
    "    except:\n",
    "        plot=None\n",
    "\n",
    "    try:\n",
    "        genres=soup.select('div[id=\"tab-genres\"] div[class=\"text-sluglist capitalize\"]')[0].p.get_text(separator='|').replace('\\n','')\n",
    "    except:\n",
    "        genres=None\n",
    "\n",
    "    try:\n",
    "        tags=soup.select('div[id=\"tab-genres\"] div[class=\"text-sluglist capitalize\"]')[1].p.get_text(separator='|').replace('\\n','')\n",
    "    except:\n",
    "        tags=None\n",
    "\n",
    "    dict={'URL':url, 'Title':title, 'Year':year, 'Original Name':original_name, 'IMDB':imdb, 'Director':director, 'Plot':plot, 'Genres':genres, 'Tags':tags}\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn=sqlite3.connect(os.path.join(path,'scraped_data','letterboxd.db'))\n",
    "cursor=conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS movie_details (\n",
    "    MOVIE_URL TEXT,\n",
    "    TITLE TEXT,\n",
    "    YEAR TEXT,\n",
    "    ORIGINAL_NAME TEXT,\n",
    "    IMDB_LINK TEXT,\n",
    "    DIRECTOR TEXT,\n",
    "    PLOT TEXT,\n",
    "    GENRES TEXT,\n",
    "    TAGS TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs scraped: 11669\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT DISTINCT MOVIE_URL FROM movie_details\")\n",
    "scraped_urls = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "print(f'URLs scraped: {len(scraped_urls)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs to scrape: 0\n"
     ]
    }
   ],
   "source": [
    "urls_to_scrape=[url for url in useful_letterboxd_urls if url not in scraped_urls]\n",
    "print(f'URLs to scrape: {len(urls_to_scrape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,movie_url in enumerate(urls_to_scrape,start=1):\n",
    "    \n",
    "    try:\n",
    "        movie_dict=return_movie_details(movie_url)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    values=[str(v) for k,v in movie_dict.items()]\n",
    "\n",
    "    cursor.execute('INSERT INTO movie_details (MOVIE_URL, TITLE, YEAR, ORIGINAL_NAME, IMDB_LINK, DIRECTOR, PLOT, GENRES, TAGS) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)', values)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f'Progress: {i}/{len(urls_to_scrape)}',end='\\r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_sql_query('SELECT * FROM movie_details',conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MOVIE_URL        0.000000\n",
       "TITLE            0.000000\n",
       "YEAR             0.001114\n",
       "ORIGINAL_NAME    0.839575\n",
       "IMDB_LINK        0.000171\n",
       "DIRECTOR         0.000428\n",
       "PLOT             0.000000\n",
       "GENRES           0.000600\n",
       "TAGS             0.083126\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.replace('None',None)\n",
    "df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PLOT']=df['PLOT'].apply(lambda x:x.split('\\nSynopsis\\n')[-1].replace('\\n','') if isinstance(x,str) else None)\n",
    "\n",
    "df['GENRES']=df['GENRES'].apply(lambda x:x.replace('| |','|') if isinstance(x,str) else None)\n",
    "\n",
    "df['TAGS']=df['TAGS'].apply(lambda x:x.replace('||','|').replace('| |','|') if isinstance(x,str) else None)\n",
    "\n",
    "df['IMDB_LINK']=df['IMDB_LINK'].str.split('maindetails').str[0]\n",
    "df['IMDB_LINK']=df['IMDB_LINK'].str.replace('http','https')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7112, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yts_urls=pd.read_csv(os.path.join(path,'scraped_data','cleaned_yts_movies_df.csv'))['IMDB_LINK'].to_list()\n",
    "df=df[df['IMDB_LINK'].isin(yts_urls)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import contractions\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia=SentimentIntensityAnalyzer()\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords=stopwords.words('english')\n",
    "stopwords_extended = stopwords+['a', 'the', 'but', 'and', 's', '.', ',', \"'s\", 'i', 'in', 'to',':',';',\"'\",'(',')','–']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(text):\n",
    "    try:\n",
    "        text = contractions.fix(text)\n",
    "        plot_tokenized = nltk.word_tokenize(text)\n",
    "        tokens_lowered = [t.lower().strip() for t in plot_tokenized]\n",
    "        removed_stopwords = [t for t in tokens_lowered if t not in stopwords_extended]\n",
    "        return list(ngrams(removed_stopwords, 2))\n",
    "    except:\n",
    "        return 'No Plot'\n",
    "\n",
    "# Extract bigrams for all plots and count them\n",
    "all_bigrams = []\n",
    "df['PLOT'].apply(lambda plot: all_bigrams.extend(get_bigrams(plot)))\n",
    "df['BIGRAMS']=df['PLOT'].apply(lambda plot:get_bigrams(plot))\n",
    "\n",
    "# Count the most common bigrams\n",
    "bigram_counts = Counter(all_bigrams)\n",
    "most_common_bigrams = bigram_counts.most_common(10000)\n",
    "most_common_bigrams=[i[0] for i in most_common_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7112/7112\r"
     ]
    }
   ],
   "source": [
    "bigrams_series=[]\n",
    "for i,bigrams_list in enumerate(df['BIGRAMS'],start=1):\n",
    "    new_bigrams_list=[]\n",
    "    for bigram in bigrams_list:\n",
    "        if bigram in most_common_bigrams:\n",
    "            new_bigrams_list.append(bigram)\n",
    "    bigrams_series.append(new_bigrams_list)\n",
    "    \n",
    "    print(f'{i}/{len(df['BIGRAMS'])}',end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BIGRAMS']=bigrams_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_texts_series=[]\n",
    "\n",
    "for i,bigrams_list in enumerate(df['BIGRAMS'],start=1):\n",
    "    bigram_texts_list=[]\n",
    "    for bigram in bigrams_list:\n",
    "        bigram_text=' '.join([w for w in bigram])\n",
    "        bigram_texts_list.append(bigram_text)\n",
    "    \n",
    "    bigram_texts_list='|'.join(i for i in bigram_texts_list)\n",
    "    bigram_texts_series.append(bigram_texts_list)\n",
    "\n",
    "df['BIGRAMS_TAGS']=bigram_texts_series\n",
    "df=df.drop(columns='BIGRAMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(path,'scraped_data','cleaned_letterboxd_movies_df.csv'),index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
